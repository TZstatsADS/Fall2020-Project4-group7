---
title: "Final_Report"
author: "Group_7"
date: "2020/11/30"
output:
  word_document: default
  html_document: default
  pdf_document: default
---
## Overview
In this project, we are going to use 5 different ways to estimate propensity scores and two algorithms to estimate ATE.
They are:

* Logistic Regression + Full Matching(propensity score distance measurement)
* L1 Logistic Reg + Full Matching(propensity score distance measurement)
* L2 Logistic Reg + Full Matching(propensity score distance measurement)
* CART + Full Matching(propensity score distance measurement)
* Boosting Stumps + Full Matching(propensity score distance measurement)
* Logistic Regression + Weighted Regression


#### Step 0.1 Load Required Packages
```{r message=FALSE, warning=FALSE}
packages.used <- c("grDevices","glmnet","rpart","gbm","MatchIt","readr","dplyr","reshape2","tidyverse","smotefamily","knitr")

#check packages that need to be installed.
packages.needed <- setdiff(packages.used, 
                           intersect(installed.packages()[,1], 
                                     packages.used))
#install additional packages
if(length(packages.needed) > 0){
  install.packages(packages.needed, dependencies = TRUE)
}
library(smotefamily)
library(grDevices)
library(glmnet)
library(rpart)
library(gbm)
library(MatchIt)
library(readr)
library(dplyr)
library(reshape2)
library(knitr)
#library(tidyverse)
```


#### Step 0.2 Import Data
```{r,results = FALSE, message=FALSE}
path = '../data/'
highdim = read_csv(paste0(path, 'highDim_dataset.csv')) #2000  187
lowdim = read_csv(paste0(path, 'lowDim_dataset.csv')) #475  24
```

### 1. Propensity Score Estimation
We define the propensity score as:
$$e(x) = Pr(T =1|X=x)$$
We assume that: 
$$0<e(x)<1$$
for all x, here we denote X as the covariates of p-dimensional vector of pre-treatment variables.

These following histograms indicate the change of different methods for propensity score estimations without and with oversampling by SMOTE on low dimensional data.
All histograms show that treatment group and control group have enough propensity scores overlapped, which indicates that both datasets are qualified to perform Propensity Score Matching and Weighted Regression.

#### 1.1 Without Oversampling for Imbalanced Classification
##### 1.1.1 Estimate by Logistic Regression
The logistic Regression model represents the class conditional probabilities through a linear function of the predictors:
$$ logit[Pr(T=1|X)] = \beta_0+\beta_1x_1+...+\beta_px_p$$ 
$$Pr(T=1|X)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_px_p)}}$$
**High dimensional data:**
```{r,echo = FALSE, results = TRUE, message=FALSE}
prop.fit.high_log_reg <- highdim[, setdiff(names(highdim), 'Y')]
start.time_ps_high_log_reg <- Sys.time()
prop.out.high_log_reg <- glm(A ~., data = prop.fit.high_log_reg,family = binomial(logit))
pscore_log_reg_h <-  prop.out.high_log_reg$fitted
end.time_ps_high_log_reg <- Sys.time()
high_cp_log_reg <- highdim
high_cp_log_reg$pscore=pscore_log_reg_h
time_ps_high_log_reg <- end.time_ps_high_log_reg - start.time_ps_high_log_reg
cat("Processing time of propensity score estimation by Logistic Regression for high dimensional data is" ,time_ps_high_log_reg, "seconds.")
```
```{r,echo = FALSE, results = TRUE}
col.alpha = function(color,alpha){
  code = col2rgb(color)/256
  rgb(code[1],code[2],code[3],alpha)
}

hist(high_cp_log_reg$pscore[high_cp_log_reg$A==1],breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,4),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(high_cp_log_reg$pscore[high_cp_log_reg$A==1]),col='red')
hist(high_cp_log_reg$pscore[high_cp_log_reg$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,4),xlim=c(0,1),add=TRUE)
lines(density(high_cp_log_reg$pscore[high_cp_log_reg$A==0]),col='blue')
```

**Low dimensional data:**
```{r,echo = FALSE, results = TRUE, message=FALSE}
prop.fit.low_log_reg <- lowdim[, setdiff(names(lowdim), 'Y')]
start.time_ps_low_log_reg <- Sys.time()
prop.out.low_log_reg <- glm(A ~., data = prop.fit.low_log_reg,family = binomial(logit))
pscore_log_reg_l <-  prop.out.low_log_reg$fitted
end.time_ps_low_log_reg <- Sys.time()
low_cp_log_reg <- lowdim
low_cp_log_reg$pscore=pscore_log_reg_l
time_ps_low_log_reg <- end.time_ps_low_log_reg - start.time_ps_low_log_reg
cat("Processing time of propensity score estimation by Logistic Regression for low dimensional data is" ,time_ps_low_log_reg, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
hist(low_cp_log_reg$pscore[low_cp_log_reg$A==1],breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_log_reg$pscore[low_cp_log_reg$A==1]),col='red')
hist(low_cp_log_reg$pscore[low_cp_log_reg$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),add=TRUE)
lines(density(low_cp_log_reg$pscore[low_cp_log_reg$A==0]),col='blue')
```

##### 1.1.2 Estimate by L1 Penalized Logistic Regression 
Regularization term is introduced to decrease the model variance in the loss function $Q$ in order to avoid overfitting of logistic regression model. For both L1 and L2 Penalized Logistic Regression, we modifying the loss function with a penalty term which effectively shrinks the estimates of the coefficients. 

Lasso Regression (Least Absolute Shrinkage and Selection Operator) with L1 norm penalty term, adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
$$Q = -\frac{1}{n}\sum_{i=1}^{n}[y_i(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})+log(1+exp(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}))]+ \lambda\sum_{j=1}^{p}|\beta_j|$$ where $Y \in \{0,1\}$

**High Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.high_l1 <- highdim[, setdiff(names(highdim), 'Y')]
x_l1 <- model.matrix(A~.,prop.fit.high_l1)[,-1]
y_l1 <- prop.fit.high_l1$A
cv.lasso <- cv.glmnet(x_l1, y_l1, alpha = 1, family = "binomial")
start.time_ps_high_l1 <- Sys.time()
prop.out.high_l1 <- glmnet(x_l1, y_l1, alpha = 1, family = "binomial", lambda = cv.lasso$lambda.min)
logit.pred_l1 <- predict(prop.out.high_l1, newx=x_l1)
pscore_l1_h <- 1 / (1 + exp(-logit.pred_l1))
end.time_ps_high_l1 <- Sys.time()
high_cp_l1 <- highdim
high_cp_l1$pscore = as.vector(pscore_l1_h)
time_ps_high_l1 <- end.time_ps_high_l1 - start.time_ps_high_l1
cat("Processing time of propensity score estimation by L1 Penalized Logistic Regression for high dimensional data is" ,time_ps_high_l1, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
col.alpha = function(color,alpha){
  code = col2rgb(color)/256
  rgb(code[1],code[2],code[3],alpha)
}
hist(high_cp_l1$pscore[high_cp_l1$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,6),xlim=c(0.2,0.8),xlab="Propensity Score", ylab="",main="")
lines(density(high_cp_l1$pscore[high_cp_l1$A==1]),col='red')
hist(high_cp_l1$pscore[high_cp_l1$A==0],breaks=40,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,6),xlim=c(0.2,0.8),add=TRUE)
lines(density(high_cp_l1$pscore[high_cp_l1$A==0]),col='blue')
```

**Low Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_l1 <- lowdim[, setdiff(names(lowdim), 'Y')]
x_l1 <- model.matrix(A~.,prop.fit.low_l1)[,-1]
y_l1 <- prop.fit.low_l1$A
cv.lasso <- cv.glmnet(x_l1, y_l1, alpha = 1, family = "binomial")
start.time_ps_low_l1 <- Sys.time()
prop.out.low_l1 <- glmnet(x_l1, y_l1, alpha = 1, family = "binomial", lambda = cv.lasso$lambda.min)
logit.pred_l1 <- predict(prop.out.low_l1, newx=x_l1)
pscore_l1_l <- 1 / (1 + exp(-logit.pred_l1))
end.time_ps_low_l1 <- Sys.time()
low_cp_l1 <- lowdim
low_cp_l1$pscore = as.vector(pscore_l1_l)
time_ps_low_l1 <- end.time_ps_low_l1 - start.time_ps_low_l1
cat("Processing time of propensity score estimation by L1 Penalized Logistic Regression for low dimensional data is" ,time_ps_low_l1, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
hist(low_cp_l1$pscore[low_cp_l1$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,8),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_l1$pscore[low_cp_l1$A==1]),col='red')
hist(low_cp_l1$pscore[low_cp_l1$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,8),add=TRUE)
lines(density(low_cp_l1$pscore[low_cp_l1$A==0]),col='blue')
```

##### 1.1.3 Estimate by L2 Penalized Logistic Regression 
Ridge regression with L2 norm penalty term adds “squared magnitude” of coefficient as penalty term to the loss function. 
$$Q = -\frac{1}{n}\sum_{i=1}^{n}[y_i(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip})+log(1+exp(\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}))]+ \lambda\sum_{j=1}^{p}{\beta_j}^2$$

**High Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.high_l2 <- highdim[, setdiff(names(highdim), 'Y')]
x_l2 <- model.matrix(A~.,prop.fit.high_l2)[,-1]
y_l2 <- prop.fit.high_l2$A
cv.ridge <- cv.glmnet(x_l2, y_l2, alpha = 0, family = "binomial")
start.time_ps_high_l2 <- Sys.time()
prop.out.high_l2 <- glmnet(x_l2, y_l2, alpha = 0, family = "binomial", lambda = cv.ridge$lambda.min)
logit.pred_l2 <- predict(prop.out.high_l2, newx=x_l2)
pscore_l2_h <- 1 / (1 + exp(-logit.pred_l2))
end.time_ps_high_l2 <- Sys.time()
high_cp_l2 <- highdim
high_cp_l2$pscore = as.vector(pscore_l2_h)
time_ps_high_l2 <- end.time_ps_high_l2 - start.time_ps_high_l2
cat("Processing time of propensity score estimation by L2 Penalized Logistic Regression for high dimensional data is" ,time_ps_high_l2, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
col.alpha = function(color,alpha){
  code = col2rgb(color)/256
  rgb(code[1],code[2],code[3],alpha)
}

hist(high_cp_l2$pscore[high_cp_l2$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,6),xlim=c(0.2,0.8),xlab="Propensity Score", ylab="",main="")
lines(density(high_cp_l2$pscore[high_cp_l2$A==1]),col='red')
hist(high_cp_l2$pscore[high_cp_l2$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,6),xlim=c(0.2,0.8),add=TRUE)
lines(density(high_cp_l2$pscore[high_cp_l2$A==0]),col='blue')
```

**Low Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_l2 <- lowdim[, setdiff(names(lowdim), 'Y')]
x_l2 <- model.matrix(A~.,prop.fit.low_l2)[,-1]
y_l2 <- prop.fit.low_l2$A
cv.ridge <- cv.glmnet(x_l2, y_l2, alpha = 0, family = "binomial")
start.time_ps_low_l2 <- Sys.time()
prop.out.low_l2 <- glmnet(x_l2, y_l2, alpha = 0, family = "binomial", lambda = cv.ridge$lambda.min)
logit.pred_l2 <- predict(prop.out.low_l2, newx=x_l2)
pscore_l2_l <- 1 / (1 + exp(-logit.pred_l2))
end.time_ps_low_l2 <- Sys.time()
low_cp_l2 <- lowdim
low_cp_l2$pscore = as.vector(pscore_l2_l)
time_ps_low_l2 <- end.time_ps_low_l2 - start.time_ps_low_l2
cat("Processing time of propensity score estimation by L2 Penalized Logistic Regression for low dimensional data is" ,time_ps_high_l2, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
hist(low_cp_l2$pscore[low_cp_l2$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,0.8),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_l2$pscore[low_cp_l2$A==1]),col='red')
hist(low_cp_l2$pscore[low_cp_l2$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,0.8),add=TRUE)
lines(density(low_cp_l2$pscore[low_cp_l2$A==0]),col='blue')
```

##### 1.1.4 Estimate by Regression Trees (CART)
Classification and regression trees could use decision tree model and provide probability of class membership. We first split the space into two regions, and model the response by the mean of $Y$ in each region. We choose the variable and split-point to achieve the best fit. Then one or both of these regions are split into two more regions, and this process in continued, until some stopping rule is applied. The corresponding regression model predicts $Y$ with a constant $c_m$ in region $R_m$, that is, $$\hat{f(x)} = \sum_{m=1}^{M}c_mI\{x \in R_m\}$$

**High Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.high_cart <- highdim[, setdiff(names(highdim), 'Y')]
high_cart <- rpart(A~., data=prop.fit.high_cart, method="class")
best_cp_high <- high_cart$cptable[which.min(high_cart$cptable[,"xerror"]),"CP"]
start.time_ps_high_CART <- Sys.time()
prop.out.high_cart <- rpart(A~., data=prop.fit.high_cart, method="class",cp=best_cp_high, parms = list(split = "information"))
pscore_cart_h <- predict(prop.out.high_cart,type='prob')[,2]
end.time_ps_high_CART <- Sys.time()
high_cp_cart<- highdim
high_cp_cart$pscore=pscore_cart_h
par(xpd = NA) # otherwise on some devices the text is clipped
plot(prop.out.high_cart)
text(prop.out.high_cart, digits = 4)
time_ps_high_CART <- end.time_ps_high_CART - start.time_ps_high_CART
cat("Processing time of propensity score estimation by Regression Trees (CART) for high dimensional data is" ,time_ps_high_CART, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
col.alpha = function(color,alpha){
  code = col2rgb(color)/256
  rgb(code[1],code[2],code[3],alpha)
}

hist(high_cp_cart$pscore[high_cp_cart$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,35),xlim=c(0.3,0.65),xlab="Propensity Score", ylab="",main="")
lines(density(high_cp_cart$pscore[high_cp_cart$A==1]),col='red')
hist(high_cp_cart$pscore[high_cp_cart$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,35),xlim=c(0.3,0.65),add=TRUE)
lines(density(high_cp_cart$pscore[high_cp_cart$A==0]),col='blue')
```

**Low Dimension Data**
```{r,echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_cart <- lowdim[, setdiff(names(lowdim), 'Y')]
low_cart <- rpart(A~., data=prop.fit.low_cart, method="class")
best_cp_low <- low_cart$cptable[which.min(low_cart$cptable[,"xerror"]),"CP"]
start.time_ps_low_CART <- Sys.time()
prop.out.low_cart <- rpart(A~., data=prop.fit.low_cart, method="class",cp=best_cp_low, parms = list(split = "information"))
pscore_cart_l <- predict(prop.out.low_cart, type='prob')[,2]
end.time_ps_low_CART <- Sys.time()
low_cp_cart<- lowdim
low_cp_cart$pscore=pscore_cart_l
par(xpd = NA) # otherwise on some devices the text is clipped
plot(prop.out.low_cart)
text(prop.out.low_cart, digits = 4)
time_ps_low_CART <- end.time_ps_low_CART - start.time_ps_low_CART
cat("Processing time of propensity score estimation by Regression Trees (CART) for low dimensional data is" ,time_ps_low_CART, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
hist(low_cp_cart$pscore[low_cp_cart$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_cart$pscore[low_cp_cart$A==1]),col='red')
hist(low_cp_cart$pscore[low_cp_cart$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),add=TRUE)
lines(density(low_cp_cart$pscore[low_cp_cart$A==0]),col='blue')
```

##### 1.1.5 Estimate by Boosting Stumps
We can represent the boosting stumps model as an addictive model: $$f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)$$
where $T(x;\theta)$ is the stump, $\Theta_m$ is the parameter of the tree stump, $M$ is the number of tree stumps.

**High Dimension Data**
```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.high_bs <- highdim[, setdiff(names(highdim), 'Y')]
gbmWithCrossValidation_high <-  gbm(formula = A~.,
                             distribution = "bernoulli",
                             data = prop.fit.high_bs,
                             n.trees = 1000,
                             shrinkage = 0.01,
                             n.minobsinnode = 20, 
                             interaction.depth=1,
                             train.fraction=1.0,
                             cv.folds = 5,
                             n.cores = 1)
bestTreeForPrediction_high <-  gbm.perf(gbmWithCrossValidation_high,plot.it=FALSE)

start.time_ps_high_BS <- Sys.time()
gbm1 <- gbm(A~.,                # predicts z from all other variables       
            data=prop.fit.high_bs,       # the dataset dropping y       
            distribution="bernoulli", # indicates logistic regression       
            n.trees=bestTreeForPrediction_high,            # runs for 500 iterations       
            shrinkage=0.01,         # sets the shrinkage parameter       
            interaction.depth=1,      # addictive model      
            bag.fraction=0.5,         # sets fraction used for Friedman's random subsampling of the data       
            train.fraction=1.0,       # train.fraction<1.0 allows for out-of-sample prediction for stopping the algorithm   
            n.minobsinnode=20
            )        # minimum node size for trees 

pscore_bs_h <-  1 / (1 + exp(-gbm1$fit))
end.time_ps_high_BS <- Sys.time()
high_cp_bs <- highdim
high_cp_bs$pscore=pscore_bs_h
time_ps_high_BS <- end.time_ps_high_BS - start.time_ps_high_BS
cat("Processing time of propensity score estimation by Boosting Stumps for high dimensional data is" ,time_ps_high_BS, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
col.alpha = function(color,alpha){
  code = col2rgb(color)/256
  rgb(code[1],code[2],code[3],alpha)
}

hist(high_cp_bs$pscore[high_cp_bs$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,6),xlab="Propensity Score", ylab="",main="")
lines(density(high_cp_bs$pscore[high_cp_bs$A==1]),col='red')
hist(high_cp_bs$pscore[high_cp_bs$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,6),add=TRUE)
lines(density(high_cp_bs$pscore[high_cp_bs$A==0]),col='blue')
```

**Low Dimension Data**
```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_bs <- lowdim[, setdiff(names(lowdim), 'Y')]
gbmWithCrossValidation_low = gbm(formula = A~.,
                             distribution = "bernoulli",
                             data = prop.fit.low_bs,
                             n.trees = 500,
                             shrinkage = 0.01,
                             n.minobsinnode = 20, 
                             interaction.depth=1,
                             train.fraction=1.0,
                             cv.folds = 5,
                             n.cores = 1)
bestTreeForPrediction_low = gbm.perf(gbmWithCrossValidation_low,plot.it=FALSE)

start.time_ps_low_BS <- Sys.time()
gbm2 <- gbm(A~.,                # predicts z from all other variables       
            data=prop.fit.low_bs,       # the dataset dropping y       
            distribution="bernoulli", # indicates logistic regression       
            n.trees=bestTreeForPrediction_low,            # runs for 95 iterations       
            shrinkage=0.01,         # sets the shrinkage parameter       
            interaction.depth=1,      # maximum allowed interaction degree       
            bag.fraction=0.5,         # sets fraction used for Friedman's random subsampling of the data       
            train.fraction=1.0,       # train.fraction<1.0 allows for out-of-sample prediction for stopping the algorithm   
            n.minobsinnode=20)        # minimum node size for trees 

pscore_bs_l <-  1 / (1 + exp(-gbm2$fit))
end.time_ps_low_BS <- Sys.time()
low_cp_bs <- lowdim
low_cp_bs$pscore=pscore_bs_l  
time_ps_low_BS <- end.time_ps_low_BS - start.time_ps_low_BS
cat("Processing time of propensity score estimation by Boosting Stumps for low dimensional data is" ,time_ps_low_BS, "seconds.")
```

```{r,echo = FALSE, results = TRUE}
hist(low_cp_bs$pscore[low_cp_bs$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,16),xlim=c(0.05,0.6),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_bs$pscore[low_cp_bs$A==1]),col='red')
hist(low_cp_bs$pscore[low_cp_bs$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,16),xlim=c(0.05,0.6),add=TRUE)
lines(density(low_cp_bs$pscore[low_cp_bs$A==0]),col='blue')
```

#### 1.2 Oversampling for Imbalanced Classification
Since the treatment classification (A) ratio is $1103 (0) : 897 (1) = 1.229654$ in high dimension data, and the treatment classification (A) ratio is $363 (0) : 112 (1) = 3.241071$ in low dimension data. We decided to use Synthetic Minority Oversampling Technique to generate synthetic positive instances using SMOTE algorithm only on low dimension data. After oversampling, the treatment classification (A) ratio is $363 (0) : 336 (1) = 1.080357$ in low dimension data.

The reason why we choose to use SMOTE is:
In full matching, reduce observations to similar pairs reduces bias allowing both groups to be equally represented and analyzed using statistical measures of significance to assess improvement for the treatment group. Even though using the propensity score for the match criteria would help dealing with imbalance data, we still want to try other oversampling techniques before full matching. We used Synthetic Minority Over-sampling Technique (SMOTE) on low dimensional data to create synthetic samples and used these generated samples to estimate the propensity scores.


```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
low_balanced_smote <- SMOTE(lowdim[,-2],unlist(lowdim[,2]),dup_size=2)$data %>% mutate(A = as.numeric(class)) %>%select(-class)
```

##### 1.2.1 Estimate by Logistic Regression
```{r, echo = FALSE, results = TRUE, message=FALSE}
prop.fit.low_balanced_smote_log_reg <- low_balanced_smote[, setdiff(names(low_balanced_smote), 'Y')]
start.time_ps_low_balanced_smote_log_reg <- Sys.time()
prop.out.low_balanced_smote_log_reg <- glm(A ~., data = prop.fit.low_balanced_smote_log_reg,family = binomial(logit))
pscore_log_reg_low_balanced_smote <-  prop.out.low_balanced_smote_log_reg$fitted
end.time_ps_low_balanced_smote_log_reg <- Sys.time()
low_balanced_smote_cp_log_reg <- low_balanced_smote
low_balanced_smote_cp_log_reg$pscore=pscore_log_reg_low_balanced_smote
time_ps_low_balanced_smote_log_reg <- end.time_ps_low_balanced_smote_log_reg - start.time_ps_low_balanced_smote_log_reg
cat("Processing time of propensity score estimation by Logistic Regression for balanced low dimensional data is" ,time_ps_low_balanced_smote_log_reg, "seconds.")

par(mfrow=c(1,2))
hist(low_cp_log_reg$pscore[low_cp_log_reg$A==1],breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_log_reg$pscore[low_cp_log_reg$A==1]),col='red')
hist(low_cp_log_reg$pscore[low_cp_log_reg$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),add=TRUE)
lines(density(low_cp_log_reg$pscore[low_cp_log_reg$A==0]),col='blue')

hist(low_balanced_smote_cp_log_reg$pscore[low_balanced_smote_cp_log_reg$A==1],breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_balanced_smote_cp_log_reg$pscore[low_balanced_smote_cp_log_reg$A==1]),col='red')
hist(low_balanced_smote_cp_log_reg$pscore[low_balanced_smote_cp_log_reg$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,7),xlim=c(0,1),add=TRUE)
lines(density(low_balanced_smote_cp_log_reg$pscore[low_balanced_smote_cp_log_reg$A==0]),col='blue')

```

##### 1.2.2 Estimate by L1 Penalized Logistic Regression 
```{r, echo = FALSE, results = TRUE, message=FALSE}
# L1 Penalized Logistic Regression
set.seed(0)
prop.fit.low_balanced_smote_l1 <- low_balanced_smote[, setdiff(names(low_balanced_smote), 'Y')]
x_l1 <- model.matrix(A~.,prop.fit.low_balanced_smote_l1)[,-1]
y_l1 <- prop.fit.low_balanced_smote_l1$A
cv.lasso_low_balanced_smote <- cv.glmnet(x_l1, y_l1, alpha = 1, family = "binomial")
start.time_ps_low_balanced_smote_l1 <- Sys.time()
prop.out.low_balanced_smote_l1 <- glmnet(x_l1, y_l1, alpha = 1, family = "binomial", lambda = cv.lasso_low_balanced_smote$lambda.min)
logit.pred_l1_low_balanced_smote <- predict(prop.out.low_balanced_smote_l1, newx=x_l1)
pscore_l1_low_balanced_smote <- 1 / (1 + exp(-logit.pred_l1_low_balanced_smote))
end.time_ps_low_balanced_smote_l1 <- Sys.time()
low_balanced_smote_cp_l1 <- low_balanced_smote
low_balanced_smote_cp_l1$pscore = as.vector(pscore_l1_low_balanced_smote)
time_ps_low_balanced_smote_l1 <- end.time_ps_low_balanced_smote_l1 - start.time_ps_low_balanced_smote_l1
cat("Processing time of propensity score estimation by L1 Penalized Logistic Regression for balanced low dimensional data is" ,time_ps_low_balanced_smote_l1, "seconds.")

par(mfrow=c(1,2))
hist(low_cp_l1$pscore[low_cp_l1$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,8),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_l1$pscore[low_cp_l1$A==1]),col='red')
hist(low_cp_l1$pscore[low_cp_l1$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,8),add=TRUE)
lines(density(low_cp_l1$pscore[low_cp_l1$A==0]),col='blue')

hist(low_balanced_smote_cp_l1$pscore[low_balanced_smote_cp_l1$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,8),xlab="Propensity Score", ylab="",main="")
lines(density(low_balanced_smote_cp_l1$pscore[low_balanced_smote_cp_l1$A==1]),col='red')
hist(low_balanced_smote_cp_l1$pscore[low_balanced_smote_cp_l1$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,8),add=TRUE)
lines(density(low_balanced_smote_cp_l1$pscore[low_balanced_smote_cp_l1$A==0]),col='blue')

```

##### 1.2.3 Estimate by L2 Penalized Logistic Regression 
```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_balanced_smote_l2 <- low_balanced_smote[, setdiff(names(low_balanced_smote), 'Y')]
x_l2 <- model.matrix(A~.,prop.fit.low_balanced_smote_l2)[,-1]
y_l2 <- prop.fit.low_balanced_smote_l2$A
cv.ridge_low_balanced_smote <- cv.glmnet(x_l2, y_l2, alpha = 0, family = "binomial")
start.time_ps_low_balanced_smote_l2 <- Sys.time()
prop.out.low_balanced_smote_l2 <- glmnet(x_l2, y_l2, alpha = 0, family = "binomial", lambda = cv.ridge_low_balanced_smote$lambda.min)
logit.pred_l2_low_balanced_smote <- predict(prop.out.low_balanced_smote_l2, newx=x_l2)
pscore_l2_low_balanced_smote <- 1 / (1 + exp(-logit.pred_l2_low_balanced_smote))
end.time_ps_low_balanced_smote_l2 <- Sys.time()
low_balanced_smote_cp_l2 <- low_balanced_smote
low_balanced_smote_cp_l2$pscore = as.vector(pscore_l2_low_balanced_smote)
time_ps_low_balanced_smote_l2 <- end.time_ps_low_balanced_smote_l2 - start.time_ps_low_balanced_smote_l2
cat("Processing time of propensity score estimation by L2 Penalized Logistic Regression for balanced low dimensional data is" ,time_ps_low_balanced_smote_l2, "seconds.")

par(mfrow=c(1,2))
hist(low_cp_l2$pscore[low_cp_l2$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,0.8),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_l2$pscore[low_cp_l2$A==1]),col='red')
hist(low_cp_l2$pscore[low_cp_l2$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,0.8),add=TRUE)
lines(density(low_cp_l2$pscore[low_cp_l2$A==0]),col='blue')

hist(low_balanced_smote_cp_l2$pscore[low_balanced_smote_cp_l2$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_balanced_smote_cp_l2$pscore[low_balanced_smote_cp_l2$A==1]),col='red')
hist(low_balanced_smote_cp_l2$pscore[low_balanced_smote_cp_l2$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,10),xlim=c(0,1),add=TRUE)
lines(density(low_balanced_smote_cp_l2$pscore[low_balanced_smote_cp_l2$A==0]),col='blue')
```

##### 1.2.4 Estimate by Regression Trees (CART)
```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_balanced_smote_cart <- low_balanced_smote[, setdiff(names(low_balanced_smote), 'Y')]
low_balanced_smote_cart <- rpart(A~., data=prop.fit.low_balanced_smote_cart, method="class")
best_cp_low_balanced_smote <- low_balanced_smote_cart$cptable[which.min(low_balanced_smote_cart$cptable[,"xerror"]),"CP"]
start.time_ps_low_balanced_smote_CART <- Sys.time()
prop.out.low_balanced_smote_cart <- rpart(A~., data=prop.fit.low_balanced_smote_cart, method="class",cp=best_cp_low_balanced_smote, parms = list(split = "information"))
pscore_cart_low_balanced_smote <- predict(prop.out.low_balanced_smote_cart, type='prob')[,2]
end.time_ps_low_balanced_smote_CART <- Sys.time()
low_balanced_smote_cp_cart<- low_balanced_smote
low_balanced_smote_cp_cart$pscore=pscore_cart_low_balanced_smote
par(xpd = NA) # otherwise on some devices the text is clipped
plot(prop.out.low_balanced_smote_cart)
text(prop.out.low_balanced_smote_cart, digits = 4)
time_ps_low_balanced_smote_CART <- end.time_ps_low_balanced_smote_CART - start.time_ps_low_balanced_smote_CART
cat("Processing time of propensity score estimation by Regression Trees (CART) for balanced low dimensional data is" ,time_ps_low_balanced_smote_CART, "seconds.")

par(mfrow=c(1,2))
hist(low_cp_cart$pscore[low_cp_cart$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_cart$pscore[low_cp_cart$A==1]),col='red')
hist(low_cp_cart$pscore[low_cp_cart$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),add=TRUE)
lines(density(low_cp_cart$pscore[low_cp_cart$A==0]),col='blue')

hist(low_balanced_smote_cp_cart$pscore[low_balanced_smote_cp_cart$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),xlab="Propensity Score", ylab="",main="")
lines(density(low_balanced_smote_cp_cart$pscore[low_balanced_smote_cp_cart$A==1]),col='red')
hist(low_balanced_smote_cp_cart$pscore[low_balanced_smote_cp_cart$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,15),xlim=c(0,1),add=TRUE)
lines(density(low_balanced_smote_cp_cart$pscore[low_balanced_smote_cp_cart$A==0]),col='blue')

```

##### 1.2.5 Estimate by Boosting Stumps
```{r, echo = FALSE, results = TRUE, message=FALSE}
set.seed(0)
prop.fit.low_balanced_smote_bs <- low_balanced_smote[, setdiff(names(low_balanced_smote), 'Y')]
gbmWithCrossValidation_low_balanced_smote = gbm(formula = A~.,
                             distribution = "bernoulli",
                             data = prop.fit.low_balanced_smote_bs,
                             n.trees = 500,
                             shrinkage = 0.01,
                             n.minobsinnode = 20, 
                             interaction.depth=1,
                             train.fraction=1.0,
                             cv.folds = 5,
                             n.cores = 1)
bestTreeForPrediction_low_balanced_smote = gbm.perf(gbmWithCrossValidation_low_balanced_smote,plot.it=FALSE)

start.time_ps_low_balanced_smote_BS <- Sys.time()
gbm2_low_balanced_smote <- gbm(A~.,                # predicts z from all other variables       
            data=prop.fit.low_balanced_smote_bs,       # the dataset dropping y       
            distribution="bernoulli", # indicates logistic regression       
            n.trees=bestTreeForPrediction_low_balanced_smote,            # runs for 95 iterations       
            shrinkage=0.01,         # sets the shrinkage parameter       
            interaction.depth=1,      # maximum allowed interaction degree       
            bag.fraction=0.5,         # sets fraction used for Friedman's random subsampling of the data       
            train.fraction=1.0,       # train.fraction<1.0 allows for out-of-sample prediction for stopping the algorithm   
            n.minobsinnode=20)        # minimum node size for trees 

pscore_bs_low_balanced_smote <-  1 / (1 + exp(-gbm2_low_balanced_smote$fit))
end.time_ps_low_balanced_smote_BS <- Sys.time()
low_balanced_smote_cp_bs <- low_balanced_smote
low_balanced_smote_cp_bs$pscore=pscore_bs_low_balanced_smote 
time_ps_low_balanced_smote_BS <- end.time_ps_low_balanced_smote_BS - start.time_ps_low_balanced_smote_BS
cat("Processing time of propensity score estimation by Boosting Stumps for balanced low dimensional data is" ,time_ps_low_balanced_smote_BS, "seconds.")

par(mfrow=c(1,2))
hist(low_cp_bs$pscore[low_cp_bs$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,16),xlim=c(0.05,0.6),xlab="Propensity Score", ylab="",main="")
lines(density(low_cp_bs$pscore[low_cp_bs$A==1]),col='red')
hist(low_cp_bs$pscore[low_cp_bs$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,16),xlim=c(0.05,0.6),add=TRUE)
lines(density(low_cp_bs$pscore[low_cp_bs$A==0]),col='blue')

hist(low_balanced_smote_cp_bs$pscore[low_balanced_smote_cp_bs$A==1], breaks=20,col=col.alpha('red',.5),freq=FALSE,
     ylim=c(0,16),xlab="Propensity Score", ylab="",main="")
lines(density(low_balanced_smote_cp_bs$pscore[low_balanced_smote_cp_bs$A==1]),col='red')
hist(low_balanced_smote_cp_bs$pscore[low_balanced_smote_cp_bs$A==0],breaks=20,col=col.alpha('blue',.5),freq=FALSE,
     ylim=c(0,16),add=TRUE)
lines(density(low_balanced_smote_cp_bs$pscore[low_balanced_smote_cp_bs$A==0]),col='blue')
```

### 2. Propensity Score Matching
After we got propensity score estimations based on different methods, we implemented Full Matching. 
First, we calculated distances of propensity scores, and then we obtained the matched sets where each matched set contains at least one treated individual and one control individual and these were formed in an optimal way. (s.t. Treated individuals who have many comparison individuals who are similar will be grouped with many comparison individuals). 
After that, we calculated subclass treatment effects for each matched set and then estimated overall ATE by an weighted average of the subclass effects where weights corresponding to the number of individuals in each subclass.

The results of estimating ATE are shown as following:
```{r, echo = FALSE, results = TRUE, message=FALSE}
full_matching_ate <- function(dat,propensity_score){
  match_full_low<-matchit(A~.-Y,data=dat,method="full",estimand = "ATE",
                          distance = propensity_score)
  data.fullMatching <- match.data(match_full_low)
  data.fullMatching$Y <- dat$Y
  a = data.fullMatching %>% group_by(subclass,A) %>% summarise(mean_y = mean(Y), .groups = 'drop')
  group_ate = a %>% group_by(subclass) %>% summarise(treat_eff = mean_y[A == 1] - mean_y[A == 0], .groups = 'drop')
  group_n = data.fullMatching %>% group_by(subclass) %>% count()
  ate = sum(group_ate$treat_eff*group_n$n/nrow(dat))
  return(ate)
}
```

```{r, echo = FALSE, results = TRUE, message=FALSE}
start.time_a <- Sys.time()
a <- full_matching_ate(highdim,pscore_log_reg_h)
end.time_a <- Sys.time()
time_a <- as.numeric(end.time_a - start.time_a)
start.time_b <- Sys.time()
b <- full_matching_ate(lowdim,pscore_log_reg_l)
end.time_b <- Sys.time()
time_b <- as.numeric(end.time_b - start.time_b)
start.time_B <- Sys.time()
B <- full_matching_ate(low_balanced_smote,pscore_log_reg_low_balanced_smote)
end.time_B <- Sys.time()
time_B <- as.numeric(end.time_B - start.time_B)
start.time_c <- Sys.time()
c <- full_matching_ate(highdim,pscore_l1_h[,1])
end.time_c <- Sys.time()
time_c <- as.numeric(end.time_c - start.time_c)
start.time_d <- Sys.time()
d <- full_matching_ate(lowdim,pscore_l1_l[,1])
end.time_d <- Sys.time()
time_d <- as.numeric(end.time_d - start.time_d)

start.time_D <- Sys.time()
D <- full_matching_ate(low_balanced_smote,pscore_l1_low_balanced_smote[,1])
end.time_D <- Sys.time()
time_D <- as.numeric(end.time_D - start.time_D)

start.time_e <- Sys.time()
e <- full_matching_ate(highdim,pscore_l2_h[,1])
end.time_e <- Sys.time()
time_e <- as.numeric(end.time_e - start.time_e)

start.time_f <- Sys.time()
f <- full_matching_ate(lowdim,pscore_l2_l[,1])
end.time_f <- Sys.time()
time_f <- as.numeric(end.time_f - start.time_f)

start.time_FF <- Sys.time()
FF <- full_matching_ate(low_balanced_smote,pscore_l2_low_balanced_smote[,1])
end.time_FF <- Sys.time()
time_FF <- as.numeric(end.time_FF - start.time_FF)

start.time_g <- Sys.time()
g <- full_matching_ate(highdim,pscore_cart_h)
end.time_g <- Sys.time()
time_g <- as.numeric(end.time_g - start.time_g)

start.time_h <- Sys.time()
h <- full_matching_ate(lowdim,pscore_cart_l)
end.time_h <- Sys.time()
time_h <- as.numeric(end.time_h - start.time_h)

start.time_H <- Sys.time()
H <- full_matching_ate(low_balanced_smote,pscore_cart_low_balanced_smote)
end.time_H <- Sys.time()
time_H <- as.numeric(end.time_H - start.time_H)

start.time_i <- Sys.time()
ii <- full_matching_ate(highdim,pscore_bs_h)
end.time_i <- Sys.time()
time_i <- as.numeric(end.time_i - start.time_i)

start.time_j <- Sys.time()
j <- full_matching_ate(lowdim,pscore_bs_l)
end.time_j <- Sys.time()
time_j <- as.numeric(end.time_j - start.time_j)

start.time_J <- Sys.time()
J <- full_matching_ate(low_balanced_smote,pscore_bs_low_balanced_smote)
end.time_J <- Sys.time()
time_J <- as.numeric(end.time_J - start.time_J)


ATE0=data.frame(Logistic = c(a,b,B),L1 = c(c,d,D),L2 = c(e,f,FF),CART = c(g,h,H),BS = c(ii,j,J),row.names = c("highdim","lowdim","balanced lowdim"))
ATE0

```


### 3. Weighted Regression
We will use the propensity score that estimated by logistic regression in part 1.1.1 and apply weighted regression to estimate ATE.
Weighted least square estimation of the regression function:
$$Y_i=\alpha_0+\tau*T_i+\alpha_1^{'}*Z_i+\alpha_2^{'}*(Z_i-\bar{Z})*T_i+\epsilon_i$$
The weight $w_i$ is:
$$w_i=\frac{T_i}{\hat{e_i}}+\frac{1-T_i}{1-\hat{e_i}}$$
where $\hat{e_i}$ is the estimated propensity score for individual i. This weighting serves to weight both the treated and control groups up to the full sample
The $Z_i$ are a subset of the covariates $X_i$ with sample average $\bar{Z}$. $\tau$ is an estimate for ATE

#### 3.1 Estimate by Logistic Regression + Weighted Regression 

**High dimensional data:**
First, we need to find $Z_i$ which is the subset of the covariates $X_i$, we will select $Z$ by estimating linear regressions:
$$Y_i=\beta_{k0}+\beta_{k1}*T_i+\beta_{k2}*X_{ik}+\epsilon_i$$
We calculate the t-statistic for the test of the null hypothesis that the slope coefficient $\beta_{k2}$ is equal to zero in each of these regressions, and now select for $Z$ all the covariates with a t-statistic larger in absolute value than 1.96. 
we will only keep covariates with t-values less than 1.96 or larger than 1.96. Therefore, the following covariates do not qualify:

```{r,echo = FALSE, results = TRUE, message=FALSE,warning = FALSE}
high=highdim
low=lowdim
# Assume all covariates have an influence on the assignment to either the control or the treatment group
lr_high <- glm(A~., data=subset(high,select=-Y), family = binomial())
# Attaching propensity score to the high_dim dataset
high$psvalue <- predict(lr_high, type='response')
# Estimation and storing weights using propensity score estimated using logistic regression
# Attaching weight to the high_dim dataset
high$weight.ATE <- ifelse(high$A == 1, 1/high$psvalue, 1/(1-high$psvalue))
# calculate the mean of each column
high_mean <- colMeans(high)
# select variables by regression
# selected by set t less than 1.96 or larger than 1.96
t_high=list()
for (i in names(high)){
  t_high[[i]] <- summary(lm(Y~get(i), high))[["coefficients"]][2, "t value"]
}
t_high = data.frame(t_high)%>%melt()%>%filter(between(value, -1.96,1.96))
t_high$variable
```

After deleting the above variables, ATE is estimated by following result, which is around -4.135:
```{r,echo = FALSE, results = TRUE, message=FALSE,warning = FALSE}

# We choose to delete the following variables
delete1 <- names(high) %in% c('V1','V2','V4','V5','V8','V9','V13','V15','V18','V28','V29','V34','V36','V39','V41','V42','V43','V44','V47','V49','V50','V52','V53','V54','V55','V57','V58','V59','V97','V151')
high_delete <- high[!delete1]
# calculate the mean of each column
high_delete_mean <- colMeans(high_delete)
# Add columns of treatment effect
high_delete_diff <- high_delete %>% 
  mutate_at(list(mean_diff = (~.*high_delete$A - high_delete_mean*high_delete$A)), .vars = vars(starts_with("V")))%>%
  select(-psvalue,-weight.ATE)

time_wlr_high = system.time(ATE_high_selected <- lm(Y~., data = high_delete_diff , weights = high_delete$weight.ATE))
out_high<-capture.output(summary(ATE_high_selected))
cat(c("[...]", out_high[9:12], "[...]"), sep = "\n")
#ATE = -4.135
#Time used to estimate ATE
#time_wlr_high[[3]]
```

**Low dimensional data:**
For low dimensional data, we follow the same steps, first is to find the subset:
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
# Assume all covariates have an influence on the assignment to either the control or the treatment group
lr_low <- glm(A~., data=subset(low,select=-Y), family = binomial())
# Attaching propensity score to the low_dim dataset
low$psvalue <- predict(lr_low, type='response')
# Estimation and storing weights using propensity score estimated using logistic regression
# Attaching weight to the low_dim dataset
low$weight.ATE <- ifelse(low$A == 1, 1/low$psvalue, 1/(1-low$psvalue))
# calculate the mean of each column
low_mean <- colMeans(low)
# select variables by regression
# selected by set t less than 1.96 or larger than 1.96
t_low=list()
for (i in names(low)){
  t_low[[i]] <- summary(lm(Y~get(i), low))[["coefficients"]][2, "t value"]
}
t_low = data.frame(t_low)%>%melt()%>%filter(between(value, -1.96,1.96))
t_low$variable
```

After deleting the above variables, ATE is estimated by following result, which is around 2.788:
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
# We choose to delete the following variables
delete <- names(low) %in% c("V2", "V4","V8","V9","V11","V14","V16","V20")
low_delete <- low[!delete]
# calculate the mean of each column
low_delete_mean <- colMeans(low_delete)
# Add columns of treatment effect
low_delete_diff <- low_delete %>% 
  mutate_at(list(mean_diff = (~.*low_delete$A - low_delete_mean*low_delete$A)), .vars = vars(starts_with("V")))%>%
  select(-psvalue,-weight.ATE)

time_wlr_low = system.time(ATE_low_selected <- lm(Y~., data = low_delete_diff , weights = low_delete$weight.ATE))
out_low<-capture.output(summary(ATE_low_selected))
cat(c("[...]", out_low[9:12], "[...]"), sep = "\n")
#ATE= 2.788
#Time used to estimate ATE
#time_wlr_low[[3]]
```
**Low dimensional balanced data:**
For low dimensional data, we follow the same steps, first is to find the subset:
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
# Assume all covariates have an influence on the assignment to either the control or the treatment group
lr_low_balanced_smote <- glm(A~., data=subset(low_balanced_smote,select=-Y), family = binomial())
# Attaching propensity score to the low_dim dataset
low_balanced_smote$psvalue <- predict(lr_low_balanced_smote, type='response')
# Estimation and storing weights using propensity score estimated using logistic regression
# Attaching weight to the low_dim dataset
low_balanced_smote$weight.ATE <- ifelse(low_balanced_smote$A == 1, 1/low_balanced_smote$psvalue, 1/(1-low_balanced_smote$psvalue))
# calculate the mean of each column
low_balanced_smote_mean <- colMeans(low_balanced_smote)
# select variables by regression
# selected by set t less than 1.96 or larger than 1.96
t_low_balanced_smote=list()
for (i in names(low_balanced_smote)){
  t_low_balanced_smote[[i]] <- summary(lm(Y~get(i), low_balanced_smote))[["coefficients"]][2, "t value"]
}
t_low_balanced_smote = data.frame(t_low_balanced_smote)%>%melt()%>%filter(between(value, -1.96,1.96))
t_low_balanced_smote$variable
```

After deleting the above variables, ATE is estimated by following result, which is around 2.929:
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
# We choose to delete the following variables
delete2 <- names(low_balanced_smote) %in% c("V4","V8","V9","V11","V14", "V20")
low_balanced_smote_delete <- low_balanced_smote[!delete2]
# calculate the mean of each column
low_balanced_smote_delete_mean <- colMeans(low_balanced_smote_delete)
# Add columns of treatment effect
low_balanced_smote_delete_diff <- low_balanced_smote_delete %>% 
  mutate_at(list(mean_diff = (~.*low_balanced_smote_delete$A - low_balanced_smote_delete_mean*low_balanced_smote_delete$A)), .vars = vars(starts_with("V")))%>%
  select(-psvalue,-weight.ATE)

time_wlr_low_balanced_smote = system.time(ATE_low_balanced_smote_selected <- lm(Y~., data = low_balanced_smote_delete_diff , weights = low_balanced_smote_delete$weight.ATE))
out_low_balanced_smote<-capture.output(summary(ATE_low_balanced_smote_selected))
cat(c("[...]", out_low_balanced_smote[c(9:10,28)], "[...]"), sep = "\n")
#ATE= 2.847
#Time used to estimate ATE
#time_wlr_low_balanced_smote[[3]]
```

### Summary
#### ATE results comparison
The following table shows estimated ATEs, we could see that Logistic Regression with Full Matching using propensity score distance measurement performs the best on both datasets. Regression tree with Full Matching using propensity score distance measurement is not well performed on low dimensional data, while the estimated ATE is very close to true ATE value after oversampling by SMOTE.
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
#ATE results
ATE1=data.frame(Logistic_FullMatching = c(a,b,B),L1_FullMatching = c(c,d,D),L2_FullMatching = c(e,f,FF),CART_FullMatching = c(g,h,H),BS_FullMatching = c(ii,j,J),Logistic_WeightedReg = c(-4.13500,2.78831,2.84753), True_ATE = c(-3,2.5,NA),row.names = c("highdim","lowdim","balanced lowdim"))
ATE1
```

#### Propensity score estimation time
The following table shows each method's time of estimating propensity scores. We could see that L1 runs fastest, which followed closely by L2. The reason might be L1 and L2 penalized strictly on dataset, therefore, they will only choose covariates that most related with. 
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
#Propensity score estimation time
TIME1=data.frame(Logistic = c(0.3910019,0.03999996,0.01700902),L1 = c(0.03896785,0.01000094,0.01003218),L2 = c(0.05304599,0.05304599,0.01000118),CART = c(1.48576,0.047000,0.08700299),BS = c(3.217024,0.073946,0.22999),row.names = c("highdim","lowdim","balanced lowdim"))
TIME1
```

#### ATE estimation time
The following table shows each method's time of estimating ATE. We could see that Weighted Regression performs much better than FullMatching. The reason might because the package MatchIt will be slower while processing large dataset. In the future, we could try different packages to see if the running time can be improved.
```{r,echo = FALSE, results = TRUE, message=FALSE, warning = FALSE}
#ATE estimation time
TIME0=data.frame(Logistic_FullMatching  = c(time_a,time_b,time_B),L1_FullMatching  = c(time_c,time_d,time_D),L2_FullMatching  = c(time_e,time_f,time_FF),CART_FullMatching  = c(time_g,time_h,time_H),BS_FullMatching  = c(time_i,time_j,time_J), Logistic_WeightedReg = c(time_wlr_high[[3]],time_wlr_low[[3]],time_wlr_low_balanced_smote[[3]]), row.names = c("highdim","lowdim","balanced lowdim"))
TIME0
```

### References

<https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk10-overview-casual-inference-methods.pdf>